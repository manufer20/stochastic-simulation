{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b013991e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Analytical Value: 1.718282\n",
      "\n",
      "--- Exercise 1: Crude Monte Carlo ---\n",
      "Point Estimate: 1.672061\n",
      "95% CI: (1.5728, 1.7713)\n",
      "CI Width: 0.198475\n",
      "\n",
      "--- Exercise 2: Antithetic Variables ---\n",
      "Point Estimate: 1.721533\n",
      "95% CI: (1.7037, 1.7393)\n",
      "CI Width: 0.035572\n",
      "\n",
      "--- Exercise 3: Control Variable ---\n",
      "Point Estimate: 1.722307\n",
      "95% CI: (1.7099, 1.7347)\n",
      "CI Width: 0.024734\n",
      "\n",
      "--- Exercise 4: Stratified Sampling ---\n",
      "Point Estimate: 1.723107\n",
      "95% CI: (1.7129, 1.7333)\n",
      "CI Width: 0.020475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# ---\n",
    "# Monte Carlo Integration Exercises\n",
    "# ---\n",
    "# This script estimates the integral of exp(x) from 0 to 1 using four different\n",
    "# Monte Carlo simulation techniques as requested in the exercises.\n",
    "\n",
    "# First, let's find the true analytical value of the integral for comparison:\n",
    "# Integral of e^x is e^x.\n",
    "# So, integral from 0 to 1 is e^1 - e^0 = exp(1) - 1\n",
    "true_value = np.exp(1) - 1\n",
    "print(f\"True Analytical Value: {true_value:.6f}\\n\")\n",
    "\n",
    "\n",
    "# Set parameters for the simulations\n",
    "n = 100  # Number of samples as specified in Exercise 1\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "\n",
    "# ---\n",
    "# 1. Crude Monte Carlo Estimator\n",
    "# ---\n",
    "# The principle is that the integral of f(x) from a to b can be estimated as\n",
    "# (b-a) * E[f(U)], where U is a uniform random variable on [a, b].\n",
    "# Since our integral is from 0 to 1, (b-a) = 1.\n",
    "# The estimator is simply the mean of f(U).\n",
    "\n",
    "print(\"--- Exercise 1: Crude Monte Carlo ---\")\n",
    "\n",
    "# Generate 100 random samples from a Uniform(0, 1) distribution\n",
    "u_crude = np.random.uniform(0, 1, n)\n",
    "\n",
    "# Apply the function e^x to each sample\n",
    "y_crude = np.exp(u_crude)\n",
    "\n",
    "# Point estimate is the mean of the results\n",
    "point_estimate_crude = np.mean(y_crude)\n",
    "\n",
    "# Calculate the standard error and the 95% confidence interval\n",
    "se_crude = np.std(y_crude, ddof=1) / np.sqrt(n)\n",
    "# Using scipy.stats.t.interval to get a more accurate CI for small samples\n",
    "ci_crude = stats.t.interval(0.95, df=n-1, loc=point_estimate_crude, scale=se_crude)\n",
    "\n",
    "print(f\"Point Estimate: {point_estimate_crude:.6f}\")\n",
    "print(f\"95% CI: ({ci_crude[0]:.4f}, {ci_crude[1]:.4f})\")\n",
    "print(f\"CI Width: {ci_crude[1] - ci_crude[0]:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ---\n",
    "# 2. Antithetic Variables\n",
    "# ---\n",
    "# This is a variance reduction technique. For each random number U, we also use its\n",
    "# \"antithetic\" counterpart, 1-U. This helps to balance out the random sampling.\n",
    "# We only need to generate n/2 random numbers to get n total evaluations.\n",
    "\n",
    "print(\"--- Exercise 2: Antithetic Variables ---\")\n",
    "\n",
    "# Use n/2 = 50 samples to keep computer resources comparable (100 total evaluations)\n",
    "m = n // 2\n",
    "u_antithetic = np.random.uniform(0, 1, m)\n",
    "\n",
    "# Calculate the function for both the original and the antithetic variables\n",
    "y1 = np.exp(u_antithetic)\n",
    "y2 = np.exp(1 - u_antithetic)\n",
    "\n",
    "# The estimator for each pair is the average of the two\n",
    "y_antithetic_pairs = (y1 + y2) / 2\n",
    "\n",
    "# The final point estimate is the mean of these paired averages\n",
    "point_estimate_antithetic = np.mean(y_antithetic_pairs)\n",
    "\n",
    "# Calculate the CI based on the m = n/2 paired samples\n",
    "se_antithetic = np.std(y_antithetic_pairs, ddof=1) / np.sqrt(m)\n",
    "ci_antithetic = stats.t.interval(0.95, df=m-1, loc=point_estimate_antithetic, scale=se_antithetic)\n",
    "\n",
    "print(f\"Point Estimate: {point_estimate_antithetic:.6f}\")\n",
    "print(f\"95% CI: ({ci_antithetic[0]:.4f}, {ci_antithetic[1]:.4f})\")\n",
    "print(f\"CI Width: {ci_antithetic[1] - ci_antithetic[0]:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ---\n",
    "# 3. Control Variable\n",
    "# ---\n",
    "# We use a second function that is highly correlated with our target function e^x,\n",
    "# but whose integral we know exactly. A good choice is g(x) = x.\n",
    "# We know Integral(x) from 0 to 1 is [x^2/2]_0^1 = 0.5.\n",
    "\n",
    "print(\"--- Exercise 3: Control Variable ---\")\n",
    "\n",
    "# We use the same 100 uniform samples as the crude method\n",
    "u_control = u_crude \n",
    "\n",
    "# Y is our variable of interest, X is the control variable\n",
    "Y = np.exp(u_control)\n",
    "X = u_control\n",
    "mu_x = 0.5 # The known true mean of the control variable\n",
    "\n",
    "# Estimate the optimal constant 'c'\n",
    "c_hat = np.cov(Y, X)[0, 1] / np.var(X)\n",
    "\n",
    "# The new control variate estimator\n",
    "Y_control = Y - c_hat * (X - mu_x)\n",
    "\n",
    "# The final point estimate is the mean of these new values\n",
    "point_estimate_control = np.mean(Y_control)\n",
    "\n",
    "# Calculate the CI based on the control variate samples\n",
    "se_control = np.std(Y_control, ddof=1) / np.sqrt(n)\n",
    "ci_control = stats.t.interval(0.95, df=n-1, loc=point_estimate_control, scale=se_control)\n",
    "\n",
    "print(f\"Point Estimate: {point_estimate_control:.6f}\")\n",
    "print(f\"95% CI: ({ci_control[0]:.4f}, {ci_control[1]:.4f})\")\n",
    "print(f\"CI Width: {ci_control[1] - ci_control[0]:.6f}\\n\")\n",
    "\n",
    "\n",
    "# ---\n",
    "# 4. Stratified Sampling\n",
    "# ---\n",
    "# We divide the interval [0, 1] into smaller, non-overlapping strata.\n",
    "# We then perform a Monte Carlo estimate in each stratum and combine the results.\n",
    "# To keep resources comparable, we'll use 10 strata with 10 samples each (100 total).\n",
    "\n",
    "print(\"--- Exercise 4: Stratified Sampling ---\")\n",
    "\n",
    "num_strata = 10\n",
    "samples_per_stratum = n // num_strata\n",
    "\n",
    "strata_means = np.zeros(num_strata)\n",
    "strata_vars = np.zeros(num_strata)\n",
    "strata_weights = 1 / num_strata # All strata have equal width (0.1)\n",
    "\n",
    "for i in range(num_strata):\n",
    "    # Lower and upper bounds of the stratum\n",
    "    lower_bound = i / num_strata\n",
    "    upper_bound = (i + 1) / num_strata\n",
    "    \n",
    "    # Generate uniform samples within the stratum\n",
    "    stratum_samples = np.random.uniform(lower_bound, upper_bound, samples_per_stratum)\n",
    "    \n",
    "    # Calculate e^x for these samples\n",
    "    y_stratum = np.exp(stratum_samples)\n",
    "    \n",
    "    # Store the mean and variance for this stratum\n",
    "    strata_means[i] = np.mean(y_stratum)\n",
    "    strata_vars[i] = np.var(y_stratum, ddof=1)\n",
    "\n",
    "# The overall point estimate is the weighted sum of the strata means\n",
    "point_estimate_stratified = np.sum(strata_weights * strata_means)\n",
    "\n",
    "# The variance of the stratified estimator is the sum of the variances of the strata estimators\n",
    "# Var(estimator) = Sum over i [ (weight_i^2 * var_i) / samples_i ]\n",
    "var_stratified = np.sum(strata_weights**2 * (strata_vars / samples_per_stratum))\n",
    "se_stratified = np.sqrt(var_stratified)\n",
    "\n",
    "# CI calculation for stratified sampling is more complex, but for large n, z-score is a good approximation\n",
    "ci_stratified = (point_estimate_stratified - 1.96 * se_stratified, \n",
    "                 point_estimate_stratified + 1.96 * se_stratified)\n",
    "\n",
    "print(f\"Point Estimate: {point_estimate_stratified:.6f}\")\n",
    "print(f\"95% CI: ({ci_stratified[0]:.4f}, {ci_stratified[1]:.4f})\")\n",
    "print(f\"CI Width: {ci_stratified[1] - ci_stratified[0]:.6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e0cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Parameters ---\n",
      "Servers (m): 10\n",
      "Offered Traffic (A): 8.00 Erlang\n",
      "Analytical Blocking Probability (Erlang-B): 0.121661\n",
      "\n",
      "--- Running Simulations ---\n",
      "Simulating 100 replications of 10000 customers each...\n",
      "Simulations complete.\n",
      "\n",
      "--- Result 1: Standard Estimator (No Control Variate) ---\n",
      "Point Estimate of Blocking Probability: 0.122478\n",
      "95% Confidence Interval: (0.121255, 0.123701)\n",
      "Confidence Interval Width: 0.002445\n",
      "\n",
      "--- Result 2: Control Variate Estimator ---\n",
      "Point Estimate of Blocking Probability: 0.121650\n",
      "95% Confidence Interval: (0.121020, 0.122281)\n",
      "Confidence Interval Width: 0.001261\n",
      "\n",
      "--- Comparison ---\n",
      "Variance Reduction: 73.42%\n",
      "The control variate method provides a more precise estimate (narrower confidence interval) for the same computational effort.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "# ---\n",
    "# Discrete-Event Simulation of a Blocking System (M/M/m/m Queue)\n",
    "# with Control Variate for Variance Reduction\n",
    "# ---\n",
    "\n",
    "# --- 1. Simulation Parameters ---\n",
    "# These are taken from the user's description of the exercises.\n",
    "m = 10                  # Number of service units (servers)\n",
    "mean_service_time = 8.0   # Mean time each customer spends with a server\n",
    "mean_interarrival_time = 1.0 # Mean time between new customers arriving\n",
    "num_customers = 10000     # Number of customers to simulate in a single run\n",
    "num_replications = 100    # Number of times we repeat the simulation to get a sample\n",
    "\n",
    "# --- 2. Analytical Solution (Erlang-B Formula) for Verification ---\n",
    "# This is the true, theoretical blocking probability for this system.\n",
    "# It allows us to check if our simulation is behaving correctly.\n",
    "A = mean_service_time / mean_interarrival_time  # Offered traffic in Erlang\n",
    "\n",
    "def erlang_b(A, m):\n",
    "    \"\"\"Calculates the blocking probability using the Erlang-B formula.\"\"\"\n",
    "    inv_b = 1.0\n",
    "    for i in range(1, m + 1):\n",
    "        inv_b = 1.0 + (i / A) * inv_b\n",
    "    return 1.0 / inv_b\n",
    "\n",
    "true_blocking_prob = erlang_b(A, m)\n",
    "print(f\"--- System Parameters ---\")\n",
    "print(f\"Servers (m): {m}\")\n",
    "print(f\"Offered Traffic (A): {A:.2f} Erlang\")\n",
    "print(f\"Analytical Blocking Probability (Erlang-B): {true_blocking_prob:.6f}\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Discrete-Event Simulation Function ---\n",
    "# This function simulates one full run of the system for a given number of customers.\n",
    "def simulate_blocking_system(m, mean_service_time, mean_interarrival_time, num_customers):\n",
    "    \"\"\"\n",
    "    Simulates a single run of the M/M/m/m blocking system.\n",
    "    \n",
    "    Returns:\n",
    "        blocking_probability (float): The fraction of customers blocked in this run.\n",
    "        empirical_offered_load (float): The measured offered load in this run.\n",
    "    \"\"\"\n",
    "    # Server_free_times keeps track of when each of the 'm' servers will become free.\n",
    "    # Initialize all servers to be free at time 0.\n",
    "    server_free_times = np.zeros(m)\n",
    "    \n",
    "    clock = 0.0\n",
    "    blocked_customers = 0\n",
    "    total_service_duration = 0.0 # To calculate empirical service time\n",
    "    \n",
    "    for i in range(num_customers):\n",
    "        # Generate the time until the next customer arrives from an exponential distribution\n",
    "        interarrival_time = np.random.exponential(mean_interarrival_time)\n",
    "        clock += interarrival_time\n",
    "        \n",
    "        # Find the server that will be free the soonest.\n",
    "        # This server is only available if it's free *before* the current customer arrives.\n",
    "        earliest_free_time = np.min(server_free_times)\n",
    "        \n",
    "        if earliest_free_time <= clock:\n",
    "            # At least one server is free. The customer is served.\n",
    "            # Find the index of the server that becomes free earliest.\n",
    "            free_server_index = np.argmin(server_free_times)\n",
    "            \n",
    "            # Generate the service time for this customer from an exponential distribution\n",
    "            service_time = np.random.exponential(mean_service_time)\n",
    "            \n",
    "            # The server now becomes busy until the service is complete.\n",
    "            server_free_times[free_server_index] = clock + service_time\n",
    "            total_service_duration += service_time\n",
    "        else:\n",
    "            # No server is free at the time of arrival. The customer is blocked.\n",
    "            blocked_customers += 1\n",
    "            \n",
    "    # --- Calculate results for this single run ---\n",
    "    blocking_probability = blocked_customers / num_customers\n",
    "    \n",
    "    # Calculate the empirical (measured) offered load for the control variate\n",
    "    # Empirical arrival rate = total customers / total time\n",
    "    empirical_lambda = num_customers / clock\n",
    "    # Empirical service time = total service duration / number of served customers\n",
    "    served_customers = num_customers - blocked_customers\n",
    "    empirical_s = total_service_duration / served_customers if served_customers > 0 else 0\n",
    "    empirical_offered_load = empirical_lambda * empirical_s\n",
    "    \n",
    "    return blocking_probability, empirical_offered_load\n",
    "\n",
    "\n",
    "# --- 4. Running the Replications ---\n",
    "# We run the simulation many times to get a distribution of results.\n",
    "print(\"--- Running Simulations ---\")\n",
    "print(f\"Simulating {num_replications} replications of {num_customers} customers each...\")\n",
    "\n",
    "# Store the results from each replication\n",
    "blocking_probabilities = np.zeros(num_replications)\n",
    "empirical_loads = np.zeros(num_replications)\n",
    "\n",
    "for i in range(num_replications):\n",
    "    prob, load = simulate_blocking_system(m, mean_service_time, mean_interarrival_time, num_customers)\n",
    "    blocking_probabilities[i] = prob\n",
    "    empirical_loads[i] = load\n",
    "\n",
    "print(\"Simulations complete.\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Standard Estimator (No Variance Reduction) ---\n",
    "# This is the \"crude\" estimate based on the average of the replications.\n",
    "print(\"--- Result 1: Standard Estimator (No Control Variate) ---\")\n",
    "mean_blocking_prob_std = np.mean(blocking_probabilities)\n",
    "se_std = np.std(blocking_probabilities, ddof=1) / np.sqrt(num_replications)\n",
    "ci_std = stats.t.interval(0.95, df=num_replications-1, loc=mean_blocking_prob_std, scale=se_std)\n",
    "\n",
    "print(f\"Point Estimate of Blocking Probability: {mean_blocking_prob_std:.6f}\")\n",
    "print(f\"95% Confidence Interval: ({ci_std[0]:.6f}, {ci_std[1]:.6f})\")\n",
    "print(f\"Confidence Interval Width: {ci_std[1] - ci_std[0]:.6f}\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Control Variate Estimator ---\n",
    "# Here, we use our knowledge of the true offered load (A=8) to improve the estimate.\n",
    "print(\"--- Result 2: Control Variate Estimator ---\")\n",
    "\n",
    "# Y is our variable of interest (what we want to estimate)\n",
    "# X is the control variate (what we are using to help)\n",
    "Y = blocking_probabilities\n",
    "X = empirical_loads\n",
    "mu_x = A  # The known true mean of the control variable (the offered load)\n",
    "\n",
    "# Estimate the optimal constant 'c' that minimizes the variance.\n",
    "# c = Cov(Y, X) / Var(X)\n",
    "c_hat = np.cov(Y, X)[0, 1] / np.var(X, ddof=1)\n",
    "\n",
    "# Apply the control variate formula to create a new, improved set of estimates\n",
    "Y_cv = Y - c_hat * (X - mu_x)\n",
    "\n",
    "# The final point estimate is the mean of these new values\n",
    "mean_blocking_prob_cv = np.mean(Y_cv)\n",
    "se_cv = np.std(Y_cv, ddof=1) / np.sqrt(num_replications)\n",
    "ci_cv = stats.t.interval(0.95, df=num_replications-1, loc=mean_blocking_prob_cv, scale=se_cv)\n",
    "\n",
    "print(f\"Point Estimate of Blocking Probability: {mean_blocking_prob_cv:.6f}\")\n",
    "print(f\"95% Confidence Interval: ({ci_cv[0]:.6f}, {ci_cv[1]:.6f})\")\n",
    "print(f\"Confidence Interval Width: {ci_cv[1] - ci_cv[0]:.6f}\\n\")\n",
    "\n",
    "# --- 7. Comparison and Conclusion ---\n",
    "print(\"--- Comparison ---\")\n",
    "variance_reduction = (se_std**2 - se_cv**2) / se_std**2\n",
    "print(f\"Variance Reduction: {variance_reduction * 100:.2f}%\")\n",
    "print(\"The control variate method provides a more precise estimate (narrower confidence interval) for the same computational effort.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
